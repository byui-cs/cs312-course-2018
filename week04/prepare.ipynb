{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/cs312.png)\n",
    "***\n",
    "\n",
    "# 04 Prepare : Reading \n",
    "\n",
    "## 1 - Objectives\n",
    "\n",
    "- This week we will be learning about detecting edges in images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2 - Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Image Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of image features** (Taken from [wikipedia](https://en.wikipedia.org/wiki/Feature_detection_(computer_vision)))\n",
    "\n",
    "\n",
    "**Edges**\n",
    "\n",
    "Edges are points where there is a boundary (or an edge) between two image regions. In general, an edge can be of almost arbitrary shape, and may include junctions. In practice, edges are usually defined as sets of points in the image which have a strong gradient magnitude. Furthermore, some common algorithms will then chain high gradient points together to form a more complete description of an edge. These algorithms usually place some constraints on the properties of an edge, such as shape, smoothness, and gradient value.\n",
    "\n",
    "Locally, edges have a one-dimensional structure.\n",
    "\n",
    "**Corners / interest points**\n",
    "\n",
    "The terms corners and interest points are used somewhat interchangeably and refer to point-like features in an image, which have a local two dimensional structure. The name \"Corner\" arose since early algorithms first performed edge detection, and then analysed the edges to find rapid changes in direction (corners). These algorithms were then developed so that explicit edge detection was no longer required, for instance by looking for high levels of curvature in the image gradient. It was then noticed that the so-called corners were also being detected on parts of the image which were not corners in the traditional sense (for instance a small bright spot on a dark background may be detected). These points are frequently known as interest points, but the term \"corner\" is used by tradition.\n",
    "\n",
    "**Blobs / regions of interest points**\n",
    "\n",
    "Blobs provide a complementary description of image structures in terms of regions, as opposed to corners that are more point-like. Nevertheless, blob descriptors may often contain a preferred point (a local maximum of an operator response or a center of gravity) which means that many blob detectors may also be regarded as interest point operators. Blob detectors can detect areas in an image which are too smooth to be detected by a corner detector.\n",
    "\n",
    "Consider shrinking an image and then performing corner detection. The detector will respond to points which are sharp in the shrunk image, but may be smooth in the original image. It is at this point that the difference between a corner detector and a blob detector becomes somewhat vague. To a large extent, this distinction can be remedied by including an appropriate notion of scale. Nevertheless, due to their response properties to different types of image structures at different scales, the LoG and DoH blob detectors are also mentioned in the article on corner detection.\n",
    "\n",
    "**Ridges**\n",
    "\n",
    "For elongated objects, the notion of ridges is a natural tool. A ridge descriptor computed from a grey-level image can be seen as a generalization of a medial axis. From a practical viewpoint, a ridge can be thought of as a one-dimensional curve that represents an axis of symmetry, and in addition has an attribute of local ridge width associated with each ridge point. Unfortunately, however, it is algorithmically harder to extract ridge features from general classes of grey-level images than edge-, corner- or blob features. Nevertheless, ridge descriptors are frequently used for road extraction in aerial images and for extracting blood vessels in medical imagesâ€”see ridge detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some information from [wikipedia](https://en.wikipedia.org/wiki/Edge_detection)\n",
    "\n",
    "> **Why it is a non-trivial task to detect edges?**\n",
    "\n",
    "> To illustrate why edge detection is not a trivial task, consider the problem of detecting edges in the following one-dimensional signal. Here, we may intuitively say that there should be an edge between the 4th and 5th pixels.\n",
    "\n",
    "> ![image.png](pixelrow1.png)\n",
    "\n",
    "> If the intensity difference were smaller between the 4th and the 5th pixels and if the intensity differences between the adjacent neighboring pixels were higher, it would not be as easy to say that there should be an edge in the corresponding region. Moreover, one could argue that this case is one in which there are several edges.\n",
    "\n",
    "> ![image.png](pixelrow2.png)\n",
    "\n",
    "> Hence, to firmly state a specific threshold on how large the intensity change between two neighbouring pixels must be for us to say that there should be an edge between these pixels is not always simple.[4] Indeed, this is one of the reasons why edge detection may be a non-trivial problem unless the objects in the scene are particularly simple and the illumination conditions can be well controlled (see for example, the edges extracted from the image with the girl above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the edges of this image.  An edge is any change in pixel values.  In the example below, notice the pixel value change from 255 to 215.  We want to calculate the different between pixels.  If the different is zero or near zero, then that indicates no change in the image (ie., no edge).  If the difference is large, then we have an edge.\n",
    "\n",
    "```text\n",
    "255 255 255 215 215 215\n",
    "255 255 255 215 215 215\n",
    "255 255 255 215 215 215\n",
    "255 255 255 215 215 215\n",
    "255 255 255 215 215 215\n",
    "```\n",
    "\n",
    "Here we caluclate the difference between two pixels and create a new image matrix of pixel values.\n",
    "\n",
    "```text\n",
    "0 0 0 40 0 0\n",
    "0 0 0 40 0 0\n",
    "0 0 0 40 0 0\n",
    "0 0 0 40 0 0\n",
    "```\n",
    "\n",
    "In the real world, images are not that clear cut and we need to compare more than just two adjacent pixels.  In order to create the new pixel value, we want to compare an area around each pixel.  This method is called convolution and uses matrixes called kernels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Sobel\n",
    "\n",
    "One type of kernel used to detect edges is called the Sobel Kernel named after Irwin Sobel.  This kernel highlights changes in pixel values.\n",
    "\n",
    "Please read [Sobel Kernel](https://en.wikipedia.org/wiki/Sobel_operator) for more details.\n",
    "\n",
    "Example image:\n",
    "![](sobel1.png)\n",
    "\n",
    "Results image:\n",
    "![](sobel2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Canny\n",
    "\n",
    "> The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images. It was developed by John F. Canny in 1986\n",
    "\n",
    "Instead of just one kernel, the Canny method uses many steps in processing an image to find edges.\n",
    "\n",
    "Please read [Canny Method](https://en.wikipedia.org/wiki/Canny_edge_detector) for more information.\n",
    "\n",
    "> The Process of Canny edge detection algorithm can be broken down to 5 different steps:\n",
    "\n",
    "> 1. Apply Gaussian filter to smooth the image in order to remove the noise\n",
    "> 1. Find the intensity gradients of the image\n",
    "> 1. Apply non-maximum suppression to get rid of spurious response to edge detection\n",
    "> 1. Apply double threshold to determine potential edges\n",
    "> 1. Track edge by hysteresis: Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges.\n",
    "\n",
    "Here is the results of using the Canny method on the image above.  Notice the differences between the Sobel and Canny results.\n",
    "![](canny.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Video Capture\n",
    "\n",
    "The next part of OpenCV that we will be learning about is how to capture video.  This capture can be from a video file or camera.\n",
    "\n",
    "The basics of capturing video is outlined in the code below.  The code example below is from the video capture link following the code sample.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# This is the start of capturing frames from a camera or video file\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Apply any filters or processing that you want to do.\n",
    "    \n",
    "    # Here, this code will convert the colors for displaying \n",
    "    # the images using OpenCV\n",
    "    \n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "```\n",
    "\n",
    "\n",
    "Here is a link to how video capture can be used in OpenCV [Video Capture link](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_gui/py_video_display/py_video_display.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sample below will open your computer camera and takes 100 frames and save them to your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# This is the start of capturing frames from a camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "while(count < 100):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    count += 1\n",
    "    cv2.imwrite('output' + str(count).zfill(3) + '.png', frame)\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3 - Reading\n",
    "\n",
    "Here is a web site that you can see how kernels work:\n",
    "\n",
    "http://setosa.io/ev/image-kernels/\n",
    "\n",
    "Links to articles\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Edge_detection\n",
    "- https://en.wikipedia.org/wiki/Digital_image_processing\n",
    "- https://en.wikipedia.org/wiki/Sobel_operator\n",
    "- https://en.wikipedia.org/wiki/Canny_edge_detector\n",
    "- https://en.wikipedia.org/wiki/Feature_detection_(computer_vision)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
